{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.lm import Vocabulary\n",
    "from PIL import Image\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_name  comment_number  \\\n",
       "0  1000092795.jpg               0   \n",
       "1  1000092795.jpg               1   \n",
       "2  1000092795.jpg               2   \n",
       "3  1000092795.jpg               3   \n",
       "4  1000092795.jpg               4   \n",
       "\n",
       "                                             comment  \n",
       "0   Two young guys with shaggy hair look at their...  \n",
       "1   Two young , White males are outside near many...  \n",
       "2   Two men in green shirts are standing in a yard .  \n",
       "3       A man in a blue shirt standing in a garden .  \n",
       "4            Two friends enjoy time spent together .  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./flickr30k_images/results.csv', delimiter='|')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['image_name', 'comment_number', 'comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_name        0\n",
      "comment_number    0\n",
      "comment           0\n",
      "dtype: int64\n",
      "0\n",
      "(158914, 3)\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "print(df.isnull().sum())\n",
    "print(df.duplicated().sum())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158914, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>two young guys with shaggy hair look at their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>two young  white males are outside near many b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>two men in green shirts are standing in a yard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>a man in a blue shirt standing in a garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>two friends enjoy time spent together</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_name comment_number  \\\n",
       "0  1000092795.jpg              0   \n",
       "1  1000092795.jpg              1   \n",
       "2  1000092795.jpg              2   \n",
       "3  1000092795.jpg              3   \n",
       "4  1000092795.jpg              4   \n",
       "\n",
       "                                             comment  \n",
       "0  two young guys with shaggy hair look at their ...  \n",
       "1  two young  white males are outside near many b...  \n",
       "2    two men in green shirts are standing in a yard   \n",
       "3        a man in a blue shirt standing in a garden   \n",
       "4             two friends enjoy time spent together   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preprocessing\n",
    "\n",
    "df['image_name'] = df['image_name'].str.strip()\n",
    "df['comment_number'] = df['comment_number'].str.strip()\n",
    "df['comment'] = df['comment'].str.strip()\n",
    "\n",
    "def remove_tags(text):\n",
    "    cleaned_text = re.sub(re.compile('<.*?>'), '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "df['comment'] = df['comment'].apply(remove_tags)\n",
    "df['comment'] = df['comment'].str.lower()\n",
    "\n",
    "exclude = string.punctuation\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))\n",
    "\n",
    "df['comment'] = df['comment'].apply(remove_punctuations)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "df['comment'] = df['comment'].apply(remove_numbers)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two young  white males are outside near many bushes '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        all_tokens = [token for sentence in sentence_list for token in self.tokenizer_eng(sentence)]\n",
    "\n",
    "        # Create a Vocabulary object from nltk.lm with the tokens and frequency threshold\n",
    "        vocab = Vocabulary(all_tokens, unk_cutoff=self.freq_threshold)\n",
    "\n",
    "        # Create mappings from word to index and index to word\n",
    "        idx = 4  # Starting index after predefined tokens\n",
    "        for word in vocab:\n",
    "            if word not in self.stoi:  # Avoid overwriting special tokens\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CustomVocabulary(freq_threshold=8)\n",
    "all_comments = df['comment'].tolist()\n",
    "vocab.build_vocabulary(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, vocab, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.df['image_name'][idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        comment = self.df['comment'][idx]\n",
    "        \n",
    "        numericalized_comment = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_comment += self.vocab.numericalize(comment)\n",
    "        numericalized_comment.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(numericalized_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        images = [item[0].unsqueeze(0) for item in batch]\n",
    "        images = torch.cat(images, dim=0)\n",
    "        comments = [item[1] for item in batch]\n",
    "        comments = pad_sequence(comments, batch_first=False, padding_value=self.pad_idx)\n",
    "        \n",
    "        return images, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(root_dir, df, vocab, transform=None, batch_size=32, shuffle=False, drop_last=True):\n",
    "    dataset = FlickrDataset(root_dir, df, vocab, transform)\n",
    "    pad_idx = vocab.stoi[\"<PAD>\"]\n",
    "    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=MyCollate(pad_idx))\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143022, 2) (15892, 2)\n"
     ]
    }
   ],
   "source": [
    "df = df[['image_name', 'comment']]\n",
    "\n",
    "train_ratio = 0.9\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=1-train_ratio, random_state=42)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143017</th>\n",
       "      <td>4827151208.jpg</td>\n",
       "      <td>a young man wearing a light blue tank top and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143018</th>\n",
       "      <td>4548479186.jpg</td>\n",
       "      <td>a woman in white with a big white flower in he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143019</th>\n",
       "      <td>519061891.jpg</td>\n",
       "      <td>a young child splashes in a green and yellow w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143020</th>\n",
       "      <td>6907188365.jpg</td>\n",
       "      <td>a man makes a diving catch during a game of ul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143021</th>\n",
       "      <td>4859164621.jpg</td>\n",
       "      <td>people use phone banks outdoors</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_name                                            comment\n",
       "143017  4827151208.jpg  a young man wearing a light blue tank top and ...\n",
       "143018  4548479186.jpg  a woman in white with a big white flower in he...\n",
       "143019   519061891.jpg  a young child splashes in a green and yellow w...\n",
       "143020  6907188365.jpg  a man makes a diving catch during a game of ul...\n",
       "143021  4859164621.jpg                   people use phone banks outdoors "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((356, 356)),\n",
    "        transforms.RandomCrop((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_loader('./flickr30k_images/flickr30k_images/', train_df, vocab, transform=transform, batch_size=64, shuffle=True, drop_last=True)\n",
    "test_loader = get_loader('./flickr30k_images/flickr30k_images/', test_df, vocab, transform=transform, batch_size=64, shuffle=False, drop_last=False)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 299, 299])\n",
      "torch.Size([60, 64])\n",
      "-----------------\n",
      "torch.Size([64, 3, 299, 299])\n",
      "torch.Size([29, 64])\n"
     ]
    }
   ],
   "source": [
    "for images, comments in train_loader:\n",
    "    print(images.shape)\n",
    "    print(comments.shape)\n",
    "    break\n",
    "\n",
    "print('-----------------')\n",
    "\n",
    "for images, comments in test_loader:\n",
    "    print(images.shape)\n",
    "    print(comments.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=True)\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.inception(images)\n",
    "        \n",
    "        for name, param in self.inception.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = self.train_CNN\n",
    "                \n",
    "        return self.dropout(self.relu(features[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        embedings = self.dropout(self.embed(captions))\n",
    "        embedings = torch.cat((features.unsqueeze(0), embedings), dim=0)\n",
    "        hidden, _ = self.lstm(embedings)\n",
    "        outputs = self.linear(hidden)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNtoRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        featured = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(featured, captions)\n",
    "        return outputs\n",
    "    \n",
    "    def caption_image(self, image, vocabulary, max_length=50):\n",
    "        result_caption = []\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens).squeeze(0)\n",
    "                predicted = output.argmax(1)\n",
    "                \n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "                \n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "                    \n",
    "        return [vocabulary.itos[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(vocab)\n",
    "num_layers=1\n",
    "learning_rate = 3e-4\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 0, Step: 0, Loss: 8.50615119934082\n",
      "1\n",
      "Epoch: 0, Step: 1, Loss: 8.496849060058594\n",
      "2\n",
      "Epoch: 0, Step: 2, Loss: 8.495614051818848\n",
      "3\n",
      "Epoch: 0, Step: 3, Loss: 8.480931282043457\n",
      "4\n",
      "Epoch: 0, Step: 4, Loss: 8.471000671386719\n",
      "5\n",
      "Epoch: 0, Step: 5, Loss: 8.45645840962728\n",
      "6\n",
      "Epoch: 0, Step: 6, Loss: 8.43936402457101\n",
      "7\n",
      "Epoch: 0, Step: 7, Loss: 8.42539393901825\n",
      "8\n",
      "Epoch: 0, Step: 8, Loss: 8.409725189208984\n",
      "9\n",
      "Epoch: 0, Step: 9, Loss: 8.392852210998536\n",
      "Epoch: 0, Step: 9, Loss: 8.392852210998536\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    step = 0\n",
    "    for idx, (imgs, captions) in enumerate(train_loader):\n",
    "        print(idx)\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        captions = captions.to(DEVICE)\n",
    "        \n",
    "        outputs = model(imgs, captions[:-1])\n",
    "        loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(loss)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        step+=1\n",
    "        \n",
    "        print(f'Epoch: {epoch}, Step: {idx}, Loss: {total_loss/step}')\n",
    "        if (idx+1)%10 == 0:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    print(f'Epoch: {epoch}, Step: {idx}, Loss: {total_loss/step}')\n",
    "    break\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
